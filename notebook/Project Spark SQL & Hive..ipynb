{
  "metadata": {
    "name": "Project Spark SQL \u0026 Hive",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#Pima Indians Diabetes dataset "
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.stat import Correlation\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Main module to execute spark code\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    conf \u003d SparkConf() #Declare spark conf variable\\\n    sc \u003d SparkContext.getOrCreate(conf\u003dconf)\n \n    #Instantiate spark builder and Set spark app name. Also, enable hive support using enableHiveSupport option of spark builder.\n    spark \u003d SparkSession(sc).builder.appName(\"Read-and-write-data-to-Hive-table-spark\").config(\"hive.metastore.uris\", \"thrift://hive-hive-metastore-1:9083\").enableHiveSupport().getOrCreate()\n \n    #Read hive table in spark using sql method of spark session class\n    df \u003d spark.sql(\"show databases\")\n \n    #Display the spark dataframe values using show method\n    df.show(10, truncate \u003d False)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#read csv file\ndatafile\u003dspark.read.csv(\"../../diabetes.csv\",header\u003dTrue,inferSchema \u003d True)\n\n#convert csv file to parquet file and store it as Hive Table\ndatafile.write.format(\"parquet\").saveAsTable(\"diabetes_tableau\")\nspark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\n\n#read Hive table using SQL\ndatafile \u003d spark.sql(\"select * from diabetes_tableau\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.toPandas().shape"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.toPandas().isnull().sum().sum()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.toPandas().head()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSelect * from diabetes_tableau limit 10"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSelect Outcome, mean(Pregnancies) as Pregnancies, mean(Glucose) as Glucose, mean(BloodPressure) as BloodPressure, mean(SkinThickness) as SkinThickness, mean(Insulin) as Insulin from diabetes_tableau group by Outcome\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.toPandas().info()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSelect Outcome, count(1) as count from diabetes_tableau group by Outcome"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\ncorrmat \u003d datafile.toPandas().corr()\nf, ax \u003d plt.subplots(figsize\u003d(6, 6))\nsns.heatmap(corrmat, vmax\u003d1, annot\u003dTrue)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\nsns.lmplot(data\u003ddatafile.toPandas(),\n           x\u003d\u0027Glucose\u0027,\n           y\u003d\u0027Age\u0027)\n\nsns.despine()\nplt.show()\nplt.clf()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\nsns.countplot(x\u003d\u0027Pregnancies\u0027,hue\u003d\u0027Outcome\u0027,data\u003ddatafile.toPandas())"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#Machine Learning Algorithms"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Train and Test data\ndata_train , data_test \u003d datafile.randomSplit([0.8,0.2], seed \u003d 123)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nassembler \u003d VectorAssembler (inputCols \u003d [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\",\"BMI\", \"DiabetesPedigreeFunction\",\"Age\"],outputCol\u003d\u0027Input Attributes\u0027)\nlogisticregressor \u003d LogisticRegression(labelCol \u003d \"Outcome\",featuresCol \u003d \u0027Input Attributes\u0027)\n\n#Pipeline\npipeline  \u003d Pipeline(stages \u003d [assembler,logisticregressor])\n\n\ndatafile.printSchema()\nModel \u003d pipeline.fit(data_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndecisiontree \u003d DecisionTreeClassifier(labelCol \u003d \"Outcome\",featuresCol \u003d \u0027Input Attributes\u0027)\n\n#Pipeline\npipelinetwo  \u003d Pipeline(stages \u003d [assembler,decisiontree])\nModeltwo \u003d pipelinetwo.fit(data_train)\n\npredtwo \u003d Modeltwo.transform(data_test)\npredtwo.toPandas().head()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nevaltwo \u003d MulticlassClassificationEvaluator(labelCol \u003d \u0027Outcome\u0027)\naccuracytwo \u003d evaltwo.evaluate(predtwo, {evaltwo.metricName:\u0027accuracy\u0027})\naccuracytwo\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrandomclassifier \u003d RandomForestClassifier(labelCol \u003d \"Outcome\",featuresCol \u003d \u0027Input Attributes\u0027)\n\n#Pipeline\npipelinethree  \u003d Pipeline(stages \u003d [assembler,randomclassifier])\n\nModelthree \u003d pipelinethree.fit(data_train)\nevalthree \u003d MulticlassClassificationEvaluator(labelCol \u003d \u0027Outcome\u0027)\npredthree \u003d Modelthree.transform(data_test)\npredthree.toPandas().head()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracythree \u003d evalthree.evaluate(predthree, {evalthree.metricName:\u0027accuracy\u0027})\naccuracythree"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#Stockage des résultats dans Hive"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nleaderboard \u003d pd.DataFrame({\u0027algorithm\u0027:[\"LogisticRegression\",\"DecisionTree\",\"Random Forest\"],\u0027metric\u0027:[\"ACCURACY\",\"ACCURACY\"],\u0027values\u0027:[rmse,accuracytwo,accuracythree]})\nprint(type(leaderboard))\n\nmetrics \u003d spark.createDataFrame(leaderboard)\n\nmetrics.write.saveAsTable(\"metric_table\")"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n\nshow tables\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf3 \u003d spark.sql(\"select * from metric_table\")\ndf3.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}